---
title: "Loss estimation homework"
author: "Maruša Oražem"
date: "5/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Setup
#### Generating toy dataset
First, we prepare the generator for our toy binary classification data, which has 8 independent variables, 3 of
which are unrelated to the target variable. Because we generate the data, we know all the properties of the
data generating process, which will allow us to study the quality of our loss estimation.
Note that we’ll be using negative log-loss (smaller is better) throughout this homework.

```{r, echo = FALSE, warning = FALSE}
library(ggplot2)

toy_data <- function(n, seed = NULL) {
set.seed(seed)
x <- matrix(rnorm(8 * n), ncol = 8)
z <- 0.4 * x[,1] - 0.5 * x[,2] + 1.75 * x[,3] - 0.2 * x[,4] + x[,5]
y <- runif(n) > 1 / (1 + exp(-z))
return (data.frame(x = x, y = y))
}
log_loss <- function(y, p) {
-(y * log(p) + (1 - y) * log(1 - p)) }
```

#### A proxy for true risk.

We’ll be using this huge dataset as a proxy for the DGP and determining the ground-truth true risk of our
models. Of course, this is itself just an estimate, but the dataset is large enough so that a model’s risk on
this dataset differs from its true risk at most on the 3rd decimal digit.

How did we determine that 100000 is enough to reduce the error to the 3rd decimal digit?
Let's say that the probabilities a model predict are uniformly distributet on the unit interval. If we generate 100000 observations and then calculate log loss with probabilities as described and then calculate the standard error we get $\frac{sd(losses)}{\sqrt(losses)} = 0.003$. This means that it differs from the mean value at the third decimal.


## HOLDOUT ESTIMATION

Holdout estimation or, as it is commonly referred to, train-test splitting, is the most common approach to
estimating a model’s risk. The first and most important thing to understand is that the model’s risk on
the test data is just an estimate of the model’s true risk. As such, it should always contain some sort of
quantification of uncertainty, such standard errors or 95% confidence intervals. We’ve learned a couple of
techniques for doing this, but in this homework we’ll use the most simple one - standard errors and 95% CI
based on the asymptotic argument that the mean loss will be normally distributed with variance n times
lower than the variance of the loss. When interpreting the results, we will also inspect the coverage of these
confidence intervals to see how this simple approach does in practice.
In the remainder of this section we will investigate some of the sources of variability and bias that contribute
to the difference between the test data risk estimate and the true risk of a model trained on the training data.

#### Model loss estimator variability due to a test data variability

We are going to use a Bernoulli-logit GLM and train it on generated toy data set with 50 observations.  After that we are going to compute its true risk on $df_dgp$ data set that consist of 100000 observations. We will generate another dataset with 50 observations, estimate the risk using this dataset, compute standard error of the estimate and record if the 95% CI contains the true risk. We are going to repeat that 1000 times. After that we are going to plot a density estimate of the differences between estimates and the true risk proxy and stated some other results.


```{r, echo = FALSE, warning = FALSE}
df_dgp <- toy_data(100000, 0)
toy <-toy_data(50,0)
h <- glm(y~., data = toy, family= binomial)
predicted <- predict(h, newdata = df_dgp[,-9], type = "response")
gt <- as.numeric(df_dgp$y)
true_risk <- mean(log_loss(gt, predicted))

```

```{r, echo = FALSE, warning = FALSE}
estimates <- c()
standard_errors <- c()
contains_true_risk <- 0
for (i in 1:1000) {
  seed <- i**2
  toy2 <- toy_data(50, seed)
  predicted <- predict(h, newdata = toy2[,-9], type = "response")
  gt <- as.numeric(toy2$y)
  losses <- log_loss(gt, predicted)
  estimates <- c(estimates, mean(losses))
  standard_error <- sd(losses)/sqrt(length(losses))
  standard_errors <- c(standard_errors, standard_error)
  contains_true_risk <- contains_true_risk + (abs(mean(losses)-true_risk) < 2*standard_error)
}

```

```{r}
x <- estimates - true_risk
plot(density(x),main = "", xlab = "est risk - true risk")

```

```{r}
print(paste0("Mean difference: ", mean(x)))
print(paste0("0.5-0.5 baseline true risk: ", mean(log_loss(gt,rep(0.5,1000)))))
print(paste0("Median standard error: ", median(standard_errors)))
print(paste0("Contain true risk: ", sum(contains_true_risk)/length(contains_true_risk)))
```

If we look at the plot we got, we can see a long tail on the right. Longer tail probably comes from the definition of the log loss function, because it rewards less than it punishes.

 We can also see that the mode is slightly less than 0 and that big amount of the differences are less then 0. That means, that most of the time we will overestimate our model, but we would not make a big error. On the other hand, the long tail tells us, that if we would underestimate it, we would do it big time.

We have used two different data sets, so they are independent of eachother. We used one for training the model and one for predicting. We have calculated the bias and we can see that it is practically equal to zero. That confirms our thought earlier that the holdout estimation is unbiased.

We can see that the median of standard errors is positive.

We can see that  for 0.5-0.5 baseline true risk is 0.6931. If we estimate the confidence interval for true risk proxy with median standard error, we can see that 0.5-0.5 baseline true risk is contained inside.

We constructed naive confidence interval. By definition, our estimate schould be in the confidence interval in 95% times. Because it is 93.4%, we are underestimate uncertainty of the estimator.

If the training set was larger, we would train out model better and that's why we would get smaller estimate of the risk. Because we would have more data, variance of the estimator would be smaller.
Similar, if we would have smaller training set, we would get bigger risk and bigger variance.

If the test set would be bigger, it would mean that when we would actually made final model, we would test it on that data also. So we would have smaller risk,  and that is why we would underestimate the performance of the model, because we would estimate bigger risk. Similar, if the test set would be smaller, we would still underestimate the model, because the data will be used at the end and the risk would be smaller but we would underestimate it less.


#### Overestimation of the deployed model's risk

In practice we rarely deploy the model trained only on the training data. Similarly, we never deploy any of
the k models that are learned during k−fold cross-validation. Instead, we use the estimation as a means to
select the best learner and then deploy the model trained on more data, typically all the available data.
More data typically means that the learner will produce a model with lower true risk. Let’s demostrate this: We are going to generate two datasets, each with 50 observations. We are goiong to train two different learner, one with only one data set and the other with both. We are going to calculate true risk proxy for each of the learners and repeat it 50 times.


```{r, echo = FALSE, warning = FALSE}
diff <- c()
for (i in 1:50) {
  seed1 <-i**2
  seed2 <-(i)**3
  toy1 <- toy_data(50,seed1)
  toy2 <- toy_data(50,seed2)
  h1 <- glm(y~., data = toy1, family= binomial)
  toy_combined <- rbind(toy1, toy2)
  h2 <- glm(y~., data = toy_combined, family= binomial)

  predicted1 <- predict(h1, newdata = df_dgp[,-9], type = "response")
  predicted2 <- predict(h2, newdata = df_dgp[,-9], type = "response")

  true_risk1 <- mean(log_loss(df_dgp$y, predicted1))
  true_risk2 <- mean(log_loss(df_dgp$y, predicted2))
  diff <- c(diff, true_risk1-true_risk2)
}
```

```{r}
summary(diff)
```
Because of the law of large numbers we know, that the model trained on more data instances, will perform better than the one on less data. We would get better performance with more data and less variance.

From observing the summary above, we can confirm our thoughts. Because estimation of the $h_1$ model has high variance, the differences get very high, which is proved with the maximum value in the summary. Meanwhile the minimum value is almost zero. The mean value is 0.6, which tells us that risk of the $h_1$ model is bigger then risk of $h_2$. As said before, this is to be expected, because models trained on more data instances, perform better and variance gets smaller.

If the data sets were larger, we would than have more data instances to train and test the model. Therefore the variance of the first model $h_1$ would decrease and the same for the variance of the $h_2$. Consequence of that would be that the differences between them would also decrease. On the other hand, if we had smaller data available, the variances would increase and if the data would be small enough, variances of both models would be high and we could not distinguish between them.



#### Loss estimator variability due to split variability

In a practical application of train-test splitting, we would choose a train-test split proportion, train on the
training data, and test on the test data. We would then use this result on the test data as an estimate of the
true risk of the model trained on all data. From the experiments so far, we can gather that this estimate will
be biased, because the tested model is trained on less data than the model we are interested in. It will also
have variance due to which observations ended up in the training set and which in the test set. To that we
can add the most basic source of variability - the variability of the losses across observations.

We are going to generate a dataset with 100 observations. We are going to use a learner on this dataset and compute true risk. Then we are going to split this dataset into test and training set and repeat it 1000 times.


```{r, echo = FALSE, warning = FALSE}
set.seed(0)
diff <- c()
diff_plot <- c()
standard_errors <- c()
contains_true_risk <- 0

toy <- toy_data(100,0)
h0 <- glm(y~., data = toy, family= binomial)
predicted0 <- predict(h0, newdata = df_dgp[,-9], type = "response")
true_risk0 <- mean(log_loss(df_dgp$y, predicted0))
  
  
for (i in 1:1000) {
  set.seed(i ** 2)
  splited <- split(toy, sample(rep(1:2,25)))
  train <- splited$'1'
  test <- splited$'2'
  h <- glm(y~., data = train, family= binomial)
  predicted <- predict(h, newdata = test[,-9], type = "response")
  losses <- log_loss(test$y, predicted)
  estimate <- mean(losses)

  standard_error <- sd(losses)/sqrt(length(losses))
  standard_errors <- c(standard_errors, standard_error)
  contains_true_risk <- contains_true_risk + (abs(mean(losses)-true_risk0) < 2*standard_error)
  diff <- c(diff, estimate-true_risk0)
  diff_plot <- c(diff_plot, estimate - 0.5755)
}


```

```{r}
ggplot(data.frame(diff_plot), aes(x=diff_plot)) + geom_density() + xlim(-0.25, 1.5) + ylim(0, 3)
```

```{r}
print(paste0("True risk proxy: ", true_risk0))
print(paste0("Average difference between estimate and true risk: ", mean(diff_plot)))
print(paste0("Median standard errros: ", median(standard_errors)))
print(paste0("Contains true risk: ", contains_true_risk))
```

From all of the calculations we have made, we can conclude many things. We can again see that the plot has mode slightly on the left and also a long tail on the right side.  We have already discussed what that means. We are underestimating the performance and overestimating the error. Plot has many extremal values. We could just have a bad test-train split.

We can also see that when we are checking if the true risk proxy is in the 95% confidence interval, because we can see that it is true less than 95% times.

If we had bigger data, estimations would have lower bias and lower true risk proxy. Because of the more data, variance of the estimator would be smaller.

If we had bigger proportion of the training data, it would mean that we could better fit the model. That would mean we would have lower bias. If the training data would be bigger, that would mean that test data would be smaller. That means we would get bigger standard error.
If we turn things around and get smaller training size and bigger test size, everything said above would be true in the contrary.


#### Cross-validation
If we extrapolate the results so far to cross-validation, we can conclude that cross-validation estimates of true
risk will also be biased and will contain a lot of variability if the dataset is relatively small. This variability
will be both due to training set and due to test set variability on top of the inherent variability of the losses.
Let’s explore this by also finally incorporating the variability caused by the draw of the dataset from the
DGP, which we have up to this point kept fixed.
We are going to create a dataset with 100 observations. We are going to use a learner on that dataset. Additionaly we are going to estimate 5 different estimators with 2-fold CV, leave one out CV, 10-fold CV, 4-fold CV and repeated 20 times 10-fold CV. We are going to repeat it 500 times.


```{r, echo = FALSE, warning = FALSE}
toy <-toy_data(100,0)
h0 <- glm(y~., data = toy, family= binomial)
predicted0 <- predict(h0, newdata = df_dgp[,-9], type = "response")
true_risk0 <- mean(log_loss(df_dgp$y, predicted0))

```

```{r, echo = FALSE, warning = FALSE}
k_CV <- function(k, data){
  all_losses <- c()
  n<- nrow(data)
  indexes <- sample(1:n, n)
  estimates <- c()
  for(fold in 1:k) {
    curr_indexes <- indexes[((fold-1)*n/k+1):(fold*n/k)]
    train <- data[-curr_indexes,]
    test <- data[curr_indexes,]
    
    h <- glm(y~., data = train, family= binomial)
    predicted <- predict(h, newdata = test[,-9], type = "response")
    
    losses <- log_loss(test$y, predicted)
    all_losses <- c(all_losses, losses)
    estimate <- mean(losses)
    estimates <- c(estimates, estimate)
  }

  return(list(all_losses, estimates))
}
```


```{r, echo = FALSE, warning = FALSE}
k_CV_special <- function(k, data){
  all_losses <- c()
  n<- nrow(data)
  indexes <- sample(1:n, n)
  estimates <- c()
  for(fold in 1:k) {
    taken <- ((fold-1)*n/k+1):(fold*n/k)
    curr_indexes <- indexes[((fold-1)*n/k+1):(fold*n/k)]
    train <- data[-curr_indexes,]
    test <- data[curr_indexes,]
    
    h <- glm(y~., data = train, family= binomial)
    predicted <- predict(h, newdata = test[,-9], type = "response")
    
    losses <- log_loss(test$y, predicted)
    all_losses <- c(all_losses, losses)
    estimate <- mean(losses)
    estimates <- c(estimates, estimate)
  }

  return(list(all_losses, estimates, indexes))
}
```

```{r, warning = FALSE, echo = FALSE}
diff_2 <- c()
diff_4 <- c()
diff_10 <- c()
diff_100 <- c()
diff_2010 <- c()

standard_error_2 <- c()
standard_error_4 <- c()
standard_error_10 <- c()
standard_error_100 <- c()
standard_error_2010 <- c()

contains_true_risk_2 <- c()
contains_true_risk_4 <- c()
contains_true_risk_10 <- c()
contains_true_risk_100 <- c()
contains_true_risk_2010 <- c()
for (i in 1:500) {
  toy <-toy_data(100,i**2)
  h0 <- glm(y~., data = toy, family= binomial)
  predicted0 <- predict(h0, newdata = df_dgp[,-9], type = "response")
  true_risk0 <- mean(log_loss(df_dgp$y, predicted0))

  #2fold CV
  result_2 <- k_CV(2,toy)
  losses_2 <- result_2[[1]]
  estimate_2 <- mean(result_2[[2]])
  curr_error <- sd(losses_2)/sqrt(length(losses_2))
  standard_error_2 <- c(standard_error_2, curr_error)
  contains_true_risk_2 <- c(contains_true_risk_2, (abs(estimate_2-true_risk0) < 2*curr_error))
  diff_2 <- c(diff_2,estimate_2-true_risk0)

  #4fold CV
  result_4 <- k_CV(4,toy)
  losses_4 <- result_4[[1]]
  estimate_4 <- mean(result_4[[2]])
  curr_error <- sd(losses_4)/sqrt(length(losses_4))
  standard_error_4 <- c(standard_error_4,curr_error)
  contains_true_risk_4 <- c(contains_true_risk_4,(abs(estimate_4-true_risk0) < 2*curr_error))
  diff_4 <- c(diff_4,estimate_4-true_risk0)

  #10fold CV
  result_10 <- k_CV(10,toy)
  losses_10 <- result_10[[1]]
  estimate_10 <- mean(result_10[[2]])
  curr_error <- sd(losses_10)/sqrt(length(losses_10))
  standard_error_10 <- c(standard_error_10,curr_error)
  contains_true_risk_10 <- c(contains_true_risk_10,(abs(estimate_10-true_risk0) < 2*curr_error))
  diff_10 <- c(diff_10,estimate_10-true_risk0)

  #100fold CV
  result_100 <- k_CV(100,toy)
  losses_100 <- result_100[[1]]
  estimate_100 <- mean(result_100[[2]])
  curr_error <- sd(losses_100)/sqrt(length(losses_100))
  standard_error_100 <- c(standard_error_100,curr_error)
  contains_true_risk_100 <- c(contains_true_risk_100,(abs(estimate_100-true_risk0) < 2*curr_error))
  diff_100 <- c(diff_100,estimate_100-true_risk0)

  #20 times 10fold CV
  all_errors_2010 <- rep(0,100)
  for(i in 1:20){
    result_10_20 <- k_CV_special(10,toy)
    losses_10_20 <- result_10_20[[1]]
    taken <- result_10_20[[3]]
    all_errors_2010 <- all_errors_2010 + losses_10_20[order(taken)]

  }
  all_errors_2010 <- all_errors_2010/20
  estimate_10_20 <- mean(all_errors_2010)
  standard_error_10_20 <- sd(all_errors_2010)/sqrt(length(all_errors_2010))
  contains_true_risk_10_20 <- as.numeric((abs(estimate_10_20-true_risk0) < 2*standard_error_10_20))
  diff_10_20 <- estimate_10_20-true_risk0
  
  contains_true_risk_2010 <- c(contains_true_risk_2010, contains_true_risk_10_20)
  diff_2010 <- c(diff_2010,diff_10_20)
  standard_error_2010 <- c(standard_error_2010, standard_error_10_20)

}


```

```{r}
print("2fold")
print(paste0("Mean difference: ", mean(diff_2)))
print(paste0("Median standard error: ", median(standard_error_2)))
print(paste0("Contain true risk: ", sum(contains_true_risk_2)/length(contains_true_risk_2)))
print("----------------------------------")
print("4fold")
print(paste0("Mean difference: ", mean(diff_4)))
print(paste0("Median standard error: ", median(standard_error_4)))
print(paste0("Contain true risk: ", sum(contains_true_risk_4)/length(contains_true_risk_4)))
print("----------------------------------")
print("10fold")
print(paste0("Mean difference: ", mean(diff_10)))
print(paste0("Median standard error: ", median(standard_error_10)))
print(paste0("Contain true risk: ", sum(contains_true_risk_10)/length(contains_true_risk_10)))
print("----------------------------------")
print("100fold")
print(paste0("Mean difference: ", mean(diff_100)))
print(paste0("Median standard error: ", median(standard_error_100)))
print(paste0("Contain true risk: ", sum(contains_true_risk_100)/length(contains_true_risk_100)))
print("----------------------------------")
print("10-20fold")
print(paste0("Mean difference: ", mean(diff_2010)))
print(paste0("Median standard error: ", median(standard_error_2010)))
print(paste0("Contain true risk: ", sum(contains_true_risk_2010)/length(contains_true_risk_2010)))
```
From the above results we can see that if we make more folds, we are performing better. By increasing the number of folds, we get lower differences and lower standard errors. If we look at the above results, we could say that leave one out is the best. But as we already know, this approach usually takes a lot of time, so we should consider in real life when we are deciding on what approach to use.



```{r}
ggplot(data.frame(diff_2), aes(x=diff_2)) + geom_density() + xlim(c(-1,10))
```

```{r}
ggplot(data.frame(diff_4), aes(x=diff_4)) + geom_density() + xlim(c(-1,10))

```
```{r}
ggplot(data.frame(diff_10), aes(x=diff_10)) + geom_density()+ xlim(c(-1,10))

```
```{r}
ggplot(data.frame(diff_2010), aes(x=diff_2010)) + geom_density() + xlim(c(-1,10))

```
```{r}
ggplot(data.frame(diff_100), aes(x=diff_100)) + geom_density() + xlim(c(-1,10))

```

If we now look at the plots, we can see that if we have more folds, the long tails on the right dissappears and also that the 'hill' is narrower. That was to be expected from the calculations and reasoning we have already made above.





#### A different scenario
In this section we are going to create a different dataset, where the results will disagree with our last experiment. We are going to construct a dataset that will have only two columns and those two will be dependent. We are also going to take a small amount of observations.


```{r, echo = FALSE, warning = FALSE}
toy_data_different <- function(n, seed = NULL) {
set.seed(seed)
x <- matrix(rnorm(n), ncol = 1)
x2 <- 2*x
y <- runif(n) > 1 / (1 + exp(-x-x2))
return (data.frame(x = cbind(x,x2), y = y))
}
```

```{r, warning = FALSE, echo = FALSE}
df_dgp <- toy_data_different(100000, 0)

diff_2 <- c()
diff_4 <- c()
diff_10 <- c()
diff_100 <- c()
diff_2010 <- c()

standard_error_2 <- c()
standard_error_4 <- c()
standard_error_10 <- c()
standard_error_100 <- c()
standard_error_2010 <- c()

contains_true_risk_2 <- c()
contains_true_risk_4 <- c()
contains_true_risk_10 <- c()
contains_true_risk_100 <- c()
contains_true_risk_2010 <- c()
for (i in 1:500) {
  toy <-toy_data_different(10,i**2)
  h0 <- glm(y~., data = toy, family= binomial)
  predicted0 <- predict(h0, newdata = df_dgp[,-9], type = "response")
  true_risk0 <- mean(log_loss(df_dgp$y, predicted0))

  #2fold CV
  result_2 <- k_CV(2,toy)
  losses_2 <- result_2[[1]]
  estimate_2 <- mean(result_2[[2]])
  curr_error <- sd(losses_2)/sqrt(length(losses_2))
  standard_error_2 <- c(standard_error_2, curr_error)
  contains_true_risk_2 <- c(contains_true_risk_2, (abs(estimate_2-true_risk0) < 2*curr_error))
  diff_2 <- c(diff_2,estimate_2-true_risk0)

  #4fold CV
  result_4 <- k_CV(4,toy)
  losses_4 <- result_4[[1]]
  estimate_4 <- mean(result_4[[2]])
  curr_error <- sd(losses_4)/sqrt(length(losses_4))
  standard_error_4 <- c(standard_error_4,curr_error)
  contains_true_risk_4 <- c(contains_true_risk_4,(abs(estimate_4-true_risk0) < 2*curr_error))
  diff_4 <- c(diff_4,estimate_4-true_risk0)

  #10fold CV
  result_10 <- k_CV(10,toy)
  losses_10 <- result_10[[1]]
  estimate_10 <- mean(result_10[[2]])
  curr_error <- sd(losses_10)/sqrt(length(losses_10))
  standard_error_10 <- c(standard_error_10,curr_error)
  contains_true_risk_10 <- c(contains_true_risk_10,(abs(estimate_10-true_risk0) < 2*curr_error))
  diff_10 <- c(diff_10,estimate_10-true_risk0)

  #100fold CV
  result_100 <- k_CV(100,toy)
  losses_100 <- result_100[[1]]
  estimate_100 <- mean(result_100[[2]])
  curr_error <- sd(losses_100)/sqrt(length(losses_100))
  standard_error_100 <- c(standard_error_100,curr_error)
  contains_true_risk_100 <- c(contains_true_risk_100,(abs(estimate_100-true_risk0) < 2*curr_error))
  diff_100 <- c(diff_100,estimate_100-true_risk0)

  #20 times 10fold CV
  all_errors_2010 <- rep(0,100)
  for(i in 1:20){
    result_10_20 <- k_CV_special(10,toy)
    losses_10_20 <- result_10_20[[1]]
    taken <- result_10_20[[3]]
    all_errors_2010 <- all_errors_2010 + losses_10_20[order(taken)]

  }
  all_errors_2010 <- all_errors_2010/20
  estimate_10_20 <- mean(all_errors_2010)
  standard_error_10_20 <- sd(all_errors_2010)/sqrt(length(all_errors_2010))
  contains_true_risk_10_20 <- as.numeric((abs(estimate_10_20-true_risk0) < 2*standard_error_10_20))
  diff_10_20 <- estimate_10_20-true_risk0

  contains_true_risk_2010 <- c(contains_true_risk_2010, contains_true_risk_10_20)
  diff_2010 <- c(diff_2010,diff_10_20)
  standard_error_2010 <- c(standard_error_2010, standard_error_10_20)

}
```

```{r}
print("2fold")
print(paste0("Mean difference: ", mean(diff_2)))
print(paste0("Median standard error: ", median(standard_error_2)))
print(paste0("Contain true risk: ", sum(contains_true_risk_2)/length(contains_true_risk_2)))
print("----------------------------------")
print("4fold")
print(paste0("Mean difference: ", mean(diff_4)))
print(paste0("Median standard error: ", median(standard_error_4)))
print(paste0("Contain true risk: ", sum(contains_true_risk_4)/length(contains_true_risk_4)))
print("----------------------------------")
print("10fold")
print(paste0("Mean difference: ", mean(diff_10)))
print(paste0("Median standard error: ", median(standard_error_10)))
print(paste0("Contain true risk: ", sum(contains_true_risk_10)/length(contains_true_risk_10)))
print("----------------------------------")
print("100fold")
print(paste0("Mean difference: ", mean(diff_100)))
print(paste0("Median standard error: ", median(standard_error_100)))
print(paste0("Contain true risk: ", sum(contains_true_risk_100)/length(contains_true_risk_100)))
print("----------------------------------")
print("10-20fold")
print(paste0("Mean difference: ", mean(diff_2010)))
print(paste0("Median standard error: ", median(standard_error_2010)))
print(paste0("Contain true risk: ", sum(contains_true_risk_2010)/length(contains_true_risk_2010)))
```
```
